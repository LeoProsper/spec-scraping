# üèóÔ∏è DIAGN√ìSTICO T√âCNICO COMPLETO - { spec64 }

**Data do Diagn√≥stico:** 28 de novembro de 2025  
**Arquiteto Respons√°vel:** GitHub Copilot (Claude Sonnet 4.5)  
**Status do Projeto:** MVP Funcional mas Fr√°gil  
**Nota Geral:** 5/10 (Produ√ß√£o-ready: ‚ùå)

---

## üìã 1. VIS√ÉO GERAL DO PROJETO

### Fluxo do Sistema (Ponta a Ponta)

```
USU√ÅRIO ‚Üí Next.js (3000)
           ‚Üì
   [Interface conversacional]
           ‚Üì
   POST /api/search/create
           ‚Üì
   [Cria registro "processing"]
           ‚Üì
   Background: processSearch()
           ‚Üì
   HTTP ‚Üí Playwright Scraper (3001)
           ‚Üì
   [Google Maps scraping]
           ‚Üì
   [Opcional: CNPJ extraction]
           ‚Üì
   [Opcional: Receita Federal API]
           ‚Üì
   Grava em Supabase (PostgreSQL)
           ‚Üì
   Update status ‚Üí "completed"
           ‚Üì
   Frontend: polling /api/search/[id]
           ‚Üì
   ResultsTable renderiza dados
```

### Estrutura do Projeto

```
novo/
‚îú‚îÄ‚îÄ apps/
‚îÇ   ‚îî‚îÄ‚îÄ web/                           # Next.js app principal
‚îÇ       ‚îú‚îÄ‚îÄ app/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ home/scout/chat/      # Interface de busca
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ api/                   # Rotas API
‚îÇ       ‚îî‚îÄ‚îÄ components/
‚îú‚îÄ‚îÄ packages/
‚îÇ   ‚îú‚îÄ‚îÄ features/kaix-scout/          # M√≥dulo de busca (hooks, services, types)
‚îÇ   ‚îú‚îÄ‚îÄ ui/                            # Shadcn components
‚îÇ   ‚îú‚îÄ‚îÄ supabase/                      # Cliente Supabase
‚îÇ   ‚îî‚îÄ‚îÄ auth/                          # Autentica√ß√£o
‚îî‚îÄ‚îÄ projeto-google-find/server/       # ‚ö†Ô∏è FORA DO MONOREPO
    ‚îú‚îÄ‚îÄ index-ultra-fast.js           # Scraper Playwright
    ‚îú‚îÄ‚îÄ cnpj-scraper.js               # CNPJ extraction
    ‚îî‚îÄ‚îÄ cnpj-enrichment.js            # Receita Federal API
```

**‚ö†Ô∏è PROBLEMA CR√çTICO:** Scraper completamente fora do monorepo

---

## üé® 2. DIAGN√ìSTICO DO FRONTEND

### ‚úÖ O QUE EST√Å BOM

1. **Arquitetura Next.js 15 moderna**
   - App Router corretamente implementado
   - React 19 com Server Components
   - Turbopack configurado para dev

2. **Estado gerenciado com TanStack Query**
   - `useSearch`, `useSearchList`, `useCreateSearch` bem estruturados
   - Polling inteligente (para enquanto status !== "processing")
   - Cache e invalida√ß√£o corretos

3. **Design System consistente**
   - Shadcn UI bem integrado
   - Tailwind v4 funcionando
   - Motion para anima√ß√µes

4. **Separa√ß√£o de responsabilidades**
   - `packages/features/kaix-scout` isola l√≥gica de neg√≥cio
   - Types centralizados em `src/types`

### ‚ö†Ô∏è O QUE EST√Å FR√ÅGIL

#### 1. ResultsTable √© um componente monstruoso (1000+ linhas)

**Arquivo:** `apps/web/app/home/scout/chat/_components/results-table.tsx`

```tsx
// 1000+ LINHAS misturando:
// - Rendering
// - L√≥gica de imagens
// - An√°lise de sentimento
// - Formata√ß√£o CNPJ
// - Extra√ß√£o WhatsApp
```

**Problema:** Imposs√≠vel de testar, debugar ou reusar  
**Impacto:** Qualquer mudan√ßa quebra m√∫ltiplas features  
**Prioridade:** üî¥ ALTA

#### 2. Acoplamento direto com API externa no service

**Arquivo:** `packages/features/kaix-scout/src/services/google-maps-scraper.service.ts`

```typescript
const SCRAPER_URL = process.env.GOOGLE_MAPS_SCRAPER_URL || 'http://localhost:3001';

export async function searchPlaces(params: SearchParams): Promise<ScraperResponse> {
  const response = await fetch(`${SCRAPER_URL}/api/scrape-maps`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(requestBody),
  });
}
```

**Problema:** 
- Hardcoded URL
- Zero abstra√ß√£o
- Sem retry logic
- Sem timeout configur√°vel

**Impacto:** Dif√≠cil mudar de provider ou adicionar resili√™ncia  
**Prioridade:** üü† M√âDIA

#### 3. Tipos duplicados/inconsistentes

```typescript
// GoogleMapsPlace vs Place vs Company
// 3 tipos diferentes representando a mesma coisa em camadas diferentes
```

**Problema:** Cast manual entre tipos, perda de type safety  
**Impacto:** Bugs em runtime que TypeScript n√£o pega  
**Prioridade:** üü° BAIXA

#### 4. Zero tratamento de erro visual

**Arquivo:** `apps/web/app/home/scout/chat/page.tsx`

```tsx
export default function ChatPage() {
  const searchParams = useSearchParams();
  const conversationId = searchParams.get('c');
  
  // ‚ö†Ô∏è Sem error boundary
  // ‚ö†Ô∏è Sem fallback UI
  // ‚ö†Ô∏è Apenas console.error
}
```

**Problema:** Usu√°rio n√£o sabe se deu erro ou est√° carregando  
**Impacto:** UX horr√≠vel quando API cai  
**Prioridade:** üî¥ ALTA

#### 5. Polling sem timeout/max retries

**Arquivo:** `packages/features/kaix-scout/src/hooks/use-search.ts`

```typescript
refetchInterval: (query) => {
  const data = query.state.data;
  if (data?.search.status === 'processing') {
    return 3000; // ‚ö†Ô∏è INFINITO - sem timeout
  }
  return false;
}
```

**Problema:** Se backend travar em "processing", frontend poll forever  
**Impacto:** Memory leak, requests infinitos  
**Prioridade:** üî¥ CR√çTICA

### üî• GAMBIARRAS CR√çTICAS

1. **"use client" em TUDO**
   - N√£o aproveita Server Components do Next.js 15
   - Tudo roda no cliente, aumenta bundle size

2. **L√≥gica complexa dentro de componentes**
   - An√°lise de sentimento dentro do CompanyItem
   - Deveria estar em hooks separados

3. **Imagens sem otimiza√ß√£o**
   - N√£o usa `next/image` corretamente
   - Carrega 10 imagens full-size do Google

---

## ‚öôÔ∏è 3. DIAGN√ìSTICO DO BACKEND/SCRAPER

### ‚úÖ O QUE EST√Å BEM FEITO

1. **Scraper V3 ultra-otimizado**
   - Playwright ao inv√©s de Puppeteer (correto)
   - Processamento paralelo em lotes de 3
   - Dual-extraction (JSON + DOM fallback)

2. **CNPJ Scraper com 3 estrat√©gias**
   - Google Search + Maps + Website
   - Valida√ß√£o de d√≠gitos verificadores

3. **Rate limiting implementado**
   ```javascript
   const RATE_LIMIT_MS = 60000; // 1 req/min
   ```

### ‚ö†Ô∏è O QUE EST√Å MUITO ERRADO

#### 1. SCRAPER COMPLETAMENTE FORA DO MONOREPO

```
novo/                    ‚Üê Monorepo Turborepo
projeto-google-find/     ‚Üê ‚ö†Ô∏è PASTA SEPARADA
```

**Problema:** 
- Zero integra√ß√£o
- Deploy manual sempre
- Types n√£o compartilhados
- Duas bases de c√≥digo separadas

**Impacto:** Quando escalar, vai precisar reescrever tudo  
**Prioridade:** üî¥ CR√çTICA

#### 2. processSearch() executa no request handler

**Arquivo:** `apps/web/app/api/search/create/route.ts`

```typescript
export async function POST(request: NextRequest) {
  // ... valida√ß√µes ...
  
  // Cria registro no banco
  const { data: search } = await supabase.from('searches').insert({...}).select().single();
  
  // ‚ö†Ô∏è "Fire and forget" sem queue, sem retry
  processSearch(search.id, searchParams).catch((error) => {
    console.error('[API] Background search processing error:', error);
  });
  
  return NextResponse.json({ data: { searchId: search.id } });
}

async function processSearch(searchId: string, params: SearchParams) {
  // Chama scraper
  // Grava no banco
  // Se cair no meio, PERDE TUDO
}
```

**Problema:** 
- "Fire and forget" sem garantias
- Se cair no meio, perde dados
- Zero observabilidade
- Sem retry autom√°tico

**Impacto:** Confiabilidade zero quando escalar  
**Prioridade:** üî¥ CR√çTICA

#### 3. Sem sistema de jobs/workers

- Background processing √© apenas `.catch()`
- N√£o tem: BullMQ, Redis, RabbitMQ, nada
- Processamento bloqueante

**Impacto:** N√£o aguenta mais que 5 usu√°rios simult√¢neos  
**Prioridade:** üî¥ CR√çTICA

#### 4. API do scraper sem autentica√ß√£o

**Arquivo:** `projeto-google-find/server/index-ultra-fast.js`

```javascript
app.post('/api/scrape-maps', async (req, res) => {
  // ‚ö†Ô∏è ZERO AUTH
  const { query, city } = req.body;
  
  if (!query) {
    return res.status(400).json({ success: false, error: 'Query is required' });
  }
  
  // Rate limiting por IP (quebra atr√°s de proxy)
  const clientIP = req.ip || req.connection.remoteAddress;
  // ...
});
```

**Problema:** 
- Qualquer um pode chamar e derrubar o servidor
- Rate limit por IP quebra atr√°s de proxy (Vercel/Nginx)

**Impacto:** DDoS trivial, custos explodindo  
**Prioridade:** üî¥ CR√çTICA

#### 5. Enriquecimento Receita Federal s√≠ncrono

**Arquivo:** `projeto-google-find/server/index-ultra-fast.js`

```javascript
if (enrichWithCNPJ) {
  await cnpjScraper.findCNPJ(); // ‚ö†Ô∏è BLOQUEIA POR 24-48s
  
  // Enriquece em lote (s√≠ncrono)
  const enrichedData = await enrichmentService.enrichBatch(cnpjs);
}
```

**Problema:** Request fica 30-50s travado  
**Impacto:** Timeout garantido quando escalar  
**Prioridade:** üî¥ ALTA

#### 6. Express standalone sem monitoramento

- Zero logs estruturados
- Sem healthcheck robusto
- Sem metrics (Prometheus, etc)
- Sem tracing

**Prioridade:** üü† M√âDIA

### üî• GAMBIARRAS CR√çTICAS

#### 1. C√≥digo JavaScript puro (n√£o TypeScript)

**Arquivo:** `index-ultra-fast.js` - 700+ linhas SEM TYPES

```javascript
// JavaScript puro em 2025
// Imposs√≠vel refatorar com seguran√ßa
```

**Impacto:** Refactoring √© Russian Roulette  
**Prioridade:** üî¥ ALTA

#### 2. Rate limiting por IP no Express

```javascript
const clientIP = req.ip || req.connection.remoteAddress;
const now = Date.now();
const lastRequest = requestTimestamps.get(clientIP);
```

**Problema:** Atr√°s de proxy (Vercel/Nginx) sempre pega mesmo IP  
**Impacto:** Rate limit quebra completamente  
**Prioridade:** üî¥ ALTA

#### 3. Browser pool n√£o gerenciado

```javascript
app.post('/api/scrape-maps', async (req, res) => {
  browser = await chromium.launch(); // Para cada request
  // ... usa browser ...
  await browser.close();
});
```

**Problema:** 
- Memory leak
- Demora 2-3s s√≥ para abrir browser
- 10 requests simult√¢neos = crash

**Impacto:** Sistema trava com carga  
**Prioridade:** üî¥ CR√çTICA

#### 4. Erro silenciado

```javascript
processSearch(search.id, searchParams).catch((error) => {
  console.error('[Background] Error:', error);
  // ‚ö†Ô∏è NOTHING ELSE - sem alertas, sem retry, sem nada
})
```

---

## üíæ 4. DIAGN√ìSTICO DO BANCO DE DADOS

### ‚úÖ O QUE EST√Å BOM

1. **Schema bem normalizado**
   - `searches` ‚Üí `companies` corretamente
   - RLS policies implementadas

2. **√çndices b√°sicos criados**
   ```sql
   CREATE INDEX idx_searches_user_id ON searches(user_id);
   CREATE INDEX idx_companies_place_id ON companies(place_id);
   ```

3. **JSONB para dados flex√≠veis**
   - `categories`, `qsa`, `top_reviews` em JSONB (correto)

### ‚ö†Ô∏è O QUE VAI QUEBRAR QUANDO ESCALAR

#### 1. companies.cnpj SEM √çNDICE

**Arquivo:** `migration-receita-fields.sql`

```sql
-- ‚ö†Ô∏è Campo criado mas SEM INDEX
ALTER TABLE companies ADD COLUMN IF NOT EXISTS cnpj TEXT;

-- √çndices criados para outros campos:
CREATE INDEX IF NOT EXISTS idx_companies_razao_social ON companies(razao_social) WHERE razao_social IS NOT NULL;
CREATE INDEX IF NOT EXISTS idx_companies_situacao_cadastral ON companies(situacao_cadastral);

-- MAS FALTA:
-- CREATE INDEX idx_companies_cnpj ON companies(cnpj) WHERE cnpj IS NOT NULL;
```

**Problema:** Buscas por CNPJ v√£o fazer full table scan  
**Impacto:** 100k+ registros = queries de 10s+  
**Prioridade:** üî¥ CR√çTICA

**Fix:**
```sql
CREATE INDEX IF NOT EXISTS idx_companies_cnpj ON companies(cnpj) WHERE cnpj IS NOT NULL;
```

#### 2. Sem particionamento

- Tabela `companies` vai ter milh√µes de linhas
- Todas queries v√£o degradar linearmente
- Backup/restore cada vez mais lento

**Impacto:** Queries lentas com volume  
**Prioridade:** üü† ALTA

**Fix sugerido:**
```sql
-- Particionar por created_at (mensal)
CREATE TABLE companies_2025_01 PARTITION OF companies 
  FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
```

#### 3. searches.results em JSONB n√£o existe

**Documenta√ß√£o menciona:** "Resultados completos em JSONB (cache inteligente)"

**Realidade:**
```sql
-- N√ÉO EXISTE NO SCHEMA
-- searches table n√£o tem campo "results"
```

**Problema:** Documenta√ß√£o mente, precisa re-scrape sempre  
**Impacto:** Performance ruim, custos altos  
**Prioridade:** üü° M√âDIA

**Fix:**
```sql
ALTER TABLE searches ADD COLUMN results JSONB DEFAULT NULL;
CREATE INDEX idx_searches_results ON searches USING gin(results) WHERE results IS NOT NULL;
```

#### 4. Sem soft delete

```sql
-- Deleta hard sempre
-- Zero auditoria
-- Sem hist√≥rico de exclus√µes
```

**Impacto:** N√£o pode recuperar dados apagados  
**Prioridade:** üü° BAIXA

**Fix:**
```sql
ALTER TABLE companies ADD COLUMN deleted_at TIMESTAMP;
CREATE INDEX idx_companies_active ON companies(deleted_at) WHERE deleted_at IS NULL;
```

#### 5. Campos da Receita Federal mal indexados

```sql
CREATE INDEX idx_companies_razao_social ON companies(razao_social) WHERE razao_social IS NOT NULL;
```

**Problema:** Partial index em TEXT sem collation  
**Impacto:** Buscas case-sensitive quebram  
**Prioridade:** üü° BAIXA

### üî• PROBLEMAS DE MODELAGEM

#### 1. companies sem user_id direto

```sql
-- Arquitetura atual:
companies ‚Üí search_id ‚Üí searches ‚Üí user_id

-- Queries precisam de JOIN triplo:
SELECT c.* 
FROM companies c
INNER JOIN searches s ON c.search_id = s.id
WHERE s.user_id = 'xxx';
```

**Problema:** JOIN triplo para pegar empresas do usu√°rio  
**Impacto:** Queries lentas, RLS policies complexas  
**Prioridade:** üü† M√âDIA

**Fix:**
```sql
ALTER TABLE companies ADD COLUMN user_id UUID REFERENCES accounts(id);
CREATE INDEX idx_companies_user_id ON companies(user_id);
```

#### 2. Dados do Google Maps + Receita Federal na mesma tabela

- 53 colunas na mesma tabela
- 27 campos Google Maps
- 26 campos Receita Federal
- Metade NULL sempre

**Problema:** Row size gigante, desperd√≠cio de espa√ßo  
**Impacto:** Scans lentos, backup pesado  
**Prioridade:** üü° M√âDIA

**Fix sugerido:**
```sql
-- Normalizar:
CREATE TABLE companies_google_data (
  company_id UUID PRIMARY KEY REFERENCES companies(id),
  -- 27 campos Google Maps
);

CREATE TABLE companies_receita_data (
  company_id UUID PRIMARY KEY REFERENCES companies(id),
  -- 26 campos Receita Federal
);
```

#### 3. Sem tabela de hist√≥rico de enriquecimento

- Se Receita Federal atualizar dados, perde hist√≥rico
- N√£o sabe quando foi √∫ltima atualiza√ß√£o
- N√£o rastreia mudan√ßas

**Prioridade:** üü° BAIXA

#### 4. place_id n√£o √© unique global

```sql
CREATE UNIQUE INDEX idx_companies_place_id ON companies(place_id);
```

**Problema:** 
- Unique apenas dentro de uma busca
- Permite duplicatas entre searches diferentes
- Mesma empresa aparece 10x no banco

**Impacto:** Dados duplicados, queries ruins  
**Prioridade:** üü† M√âDIA

**Fix:**
```sql
-- Remover √≠ndice atual e recriar global
DROP INDEX idx_companies_place_id;
CREATE UNIQUE INDEX idx_companies_place_id_global ON companies(place_id);
```

---

## üßπ 5. QUALIDADE DE C√ìDIGO

### N√≠vel: **M√âDIA-BAIXA** (5/10)

### Problemas Principais

#### 1. Zero testes
- Nem unit, nem integration, nem E2E
- Imposs√≠vel refatorar com seguran√ßa
- Cada mudan√ßa √© um salto de f√©

**Prioridade:** üî¥ ALTA

#### 2. Mistura de padr√µes
- JavaScript no scraper
- TypeScript no Next.js
- Sem lint rules compartilhadas
- Sem code style consistency

**Prioridade:** üü° BAIXA

#### 3. Coment√°rios em portugu√™s

```javascript
// ‚úÖ Adiciona todos os campos da Receita Federal
// ‚ö° 2-3 segundos para 12 lugares
```

**Problema:** C√≥digo internacional com docs localizadas  
**Prioridade:** üü° BAIXA

#### 4. README desatualizado

- Menciona TanStack Table mas n√£o √© usado de verdade
- Instru√ß√µes de deploy quebradas
- Comandos desatualizados

**Prioridade:** üü° BAIXA

#### 5. Sem CI/CD

- Zero automation
- Deploy manual
- Sem testes autom√°ticos
- Sem quality gates

**Prioridade:** üü† M√âDIA

#### 6. Logs n√£o estruturados

```javascript
console.log('‚úÖ JSON');
console.log('üîÑ DOM');
console.log(`üìä CNPJs encontrados: ${cnpjCount}/${businesses.length}`);
```

**Problema:** 
- Emojis em prod
- Imposs√≠vel parsear logs
- Sem structured logging
- Dif√≠cil debugar produ√ß√£o

**Prioridade:** üü† M√âDIA

**Fix:**
```typescript
// Usar Pino ou Winston
import pino from 'pino';

const logger = pino({
  level: 'info',
  transport: { target: 'pino-pretty' }
});

logger.info({ method: 'json', success: true }, 'Data extracted');
logger.error({ error: err.message, stack: err.stack }, 'Scraping failed');
```

---

## üö® 6. RISCOS CR√çTICOS - "SE N√ÉO ARRUMAR AGORA, VAI DOER"

### üî¥ CR√çTICO (Quebra em produ√ß√£o)

#### 1. Scraper n√£o aguenta concorr√™ncia
- Browser pool zero gerenciamento
- 5 requests simult√¢neos = crash
- Memory leak garantido

**A√ß√£o:** Implementar browser pool + queue AGORA

```typescript
// playwright-pool.ts
import { chromium, Browser } from 'playwright';

class BrowserPool {
  private browsers: Browser[] = [];
  private maxSize: number;
  
  constructor(maxSize = 3) {
    this.maxSize = maxSize;
  }
  
  async acquire(): Promise<Browser> {
    if (this.browsers.length < this.maxSize) {
      const browser = await chromium.launch({ headless: true });
      this.browsers.push(browser);
      return browser;
    }
    // Wait for available browser
    await new Promise(resolve => setTimeout(resolve, 1000));
    return this.acquire();
  }
  
  async release(browser: Browser) {
    // Keep browser alive for reuse
  }
}
```

#### 2. processSearch() sem retry/queue
- Perde dados se cair no meio
- Zero observabilidade
- Sem garantias de processamento

**A√ß√£o:** BullMQ + Redis imediato

```typescript
// jobs/scraping-queue.ts
import Queue from 'bull';

const scrapingQueue = new Queue('scraping', {
  redis: { host: 'localhost', port: 6379 }
});

scrapingQueue.process(async (job) => {
  const { searchId, params } = job.data;
  
  try {
    await processSearch(searchId, params);
  } catch (error) {
    // Retry autom√°tico via Bull
    throw error;
  }
});

// Em route.ts:
await scrapingQueue.add({ searchId: search.id, params });
```

#### 3. API scraper sem auth
- Qualquer um pode atacar
- DDoS trivial

**A√ß√£o:** JWT ou API keys imediato

```typescript
// middleware/auth.ts
import jwt from 'jsonwebtoken';

export function validateApiKey(req, res, next) {
  const apiKey = req.headers['x-api-key'];
  
  if (!apiKey || !jwt.verify(apiKey, process.env.JWT_SECRET)) {
    return res.status(401).json({ error: 'Unauthorized' });
  }
  
  next();
}

// index-ultra-fast.js:
app.post('/api/scrape-maps', validateApiKey, async (req, res) => {
  // ...
});
```

#### 4. Frontend polling infinito
- Memory leak garantido
- Requests sem fim

**A√ß√£o:** Timeout + max retries

```typescript
// use-search.ts
refetchInterval: (query) => {
  const data = query.state.data;
  const attempt = query.state.fetchFailureCount;
  
  // ‚úÖ MAX 40 retries (2 minutos)
  if (attempt > 40) {
    console.error('Search timeout after 2 minutes');
    return false;
  }
  
  if (data?.search.status === 'processing') {
    return 3000;
  }
  
  return false;
}
```

### üü† ALTO (Vai explodir quando crescer)

#### 5. Scraper fora do monorepo
- Deploy manual sempre
- Types n√£o compartilhados

**A√ß√£o:** Migrar para `apps/scraper` TypeScript

```
novo/
‚îú‚îÄ‚îÄ apps/
‚îÇ   ‚îú‚îÄ‚îÄ web/
‚îÇ   ‚îî‚îÄ‚îÄ scraper/          # ‚Üê MIGRAR AQUI
‚îÇ       ‚îú‚îÄ‚îÄ src/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ index.ts
‚îÇ       ‚îî‚îÄ‚îÄ package.json
```

#### 6. companies sem √≠ndice em cnpj
- Queries v√£o travar

**A√ß√£o:** `CREATE INDEX idx_companies_cnpj`

```sql
CREATE INDEX IF NOT EXISTS idx_companies_cnpj 
ON companies(cnpj) 
WHERE cnpj IS NOT NULL;
```

#### 7. ResultsTable 1000+ linhas
- Imposs√≠vel manter

**A√ß√£o:** Quebrar em 10+ componentes

```
results-table.tsx (1000 linhas)
‚Üì
‚îú‚îÄ‚îÄ CompanyCard.tsx (150 linhas)
‚îú‚îÄ‚îÄ CompanyDetails.tsx (200 linhas)
‚îú‚îÄ‚îÄ ReviewsAnalysis.tsx (150 linhas)
‚îú‚îÄ‚îÄ ReviewsSummary.tsx (100 linhas)
‚îú‚îÄ‚îÄ ImageGallery.tsx (80 linhas)
‚îî‚îÄ‚îÄ ActionButtons.tsx (50 linhas)
```

#### 8. Sem particionamento DB
- 1M+ registros = lento

**A√ß√£o:** Partition por created_at

```sql
-- Converter para particionada
CREATE TABLE companies_partitioned (
  LIKE companies INCLUDING ALL
) PARTITION BY RANGE (created_at);

CREATE TABLE companies_2025_11 PARTITION OF companies_partitioned
  FOR VALUES FROM ('2025-11-01') TO ('2025-12-01');
```

### üü° M√âDIO (D√≠vida t√©cnica)

#### 9. Zero testes
- Medo de mexer

**A√ß√£o:** Vitest + testes cr√≠ticos

```typescript
// use-search.test.ts
import { renderHook, waitFor } from '@testing-library/react';
import { useSearch } from './use-search';

describe('useSearch', () => {
  it('should poll until completed', async () => {
    const { result } = renderHook(() => useSearch('search-id'));
    
    await waitFor(() => {
      expect(result.current.data?.search.status).toBe('completed');
    });
  });
});
```

#### 10. Logs n√£o estruturados
- Imposs√≠vel debugar prod

**A√ß√£o:** Pino logger + JSON

```typescript
import pino from 'pino';

const logger = pino({
  level: process.env.LOG_LEVEL || 'info',
});

logger.info({ searchId, query }, 'Search started');
logger.error({ error: err.message }, 'Scraping failed');
```

#### 11. Tipos duplicados
- Cast manual everywhere

**A√ß√£o:** Unificar types

```typescript
// @kit/types/company.ts
export interface Company {
  // Uni√£o de GoogleMapsPlace + ReceitaFederalData + Company
  id: string;
  place_id: string;
  name: string;
  // ... todos campos unificados
}
```

#### 12. Sem soft delete
- Zero auditoria

**A√ß√£o:** `deleted_at` column

```sql
ALTER TABLE companies ADD COLUMN deleted_at TIMESTAMP;
ALTER TABLE searches ADD COLUMN deleted_at TIMESTAMP;
```

---

## üóìÔ∏è 7. ROADMAP T√âCNICO 15-30 DIAS

### Semana 1: Estabiliza√ß√£o Backend

**Objetivo:** Backend aguenta 50+ usu√°rios simult√¢neos

#### Dia 1-2: Migrar scraper para TypeScript
```
ANTES:
projeto-google-find/server/index-ultra-fast.js (700 linhas JS)

DEPOIS:
apps/scraper/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scrape.route.ts
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ google-maps.service.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cnpj.service.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ receita.service.ts
‚îÇ   ‚îú‚îÄ‚îÄ types/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.ts
‚îÇ   ‚îî‚îÄ‚îÄ index.ts
‚îî‚îÄ‚îÄ package.json
```

**Benef√≠cios:**
- Type safety completo
- Compartilha types com frontend
- Deploy integrado no monorepo

#### Dia 3-4: Implementar job queue
```typescript
// apps/scraper/src/queue/scraping.queue.ts
import Queue from 'bull';

export const scrapingQueue = new Queue('scraping', {
  redis: process.env.REDIS_URL,
  settings: {
    maxStalledCount: 3,
    stalledInterval: 30000,
  }
});

scrapingQueue.process(5, async (job) => {
  const { searchId, params } = job.data;
  
  await processSearchJob(searchId, params);
});

// apps/web/app/api/search/create/route.ts
await scrapingQueue.add(
  { searchId: search.id, params },
  { attempts: 3, backoff: 5000 }
);
```

**Benef√≠cios:**
- Retry autom√°tico
- Observabilidade (Bull Board)
- Controle de concorr√™ncia

#### Dia 5: Browser pool gerenciado
```typescript
// apps/scraper/src/services/browser-pool.ts
import { chromium, Browser, BrowserContext } from 'playwright';

class BrowserPool {
  private pool: Browser[] = [];
  private inUse = new Set<Browser>();
  private readonly maxSize = 3;
  
  async acquire(): Promise<BrowserContext> {
    let browser = this.pool.find(b => !this.inUse.has(b));
    
    if (!browser) {
      if (this.pool.length < this.maxSize) {
        browser = await chromium.launch({ headless: true });
        this.pool.push(browser);
      } else {
        // Wait for available
        await new Promise(resolve => setTimeout(resolve, 1000));
        return this.acquire();
      }
    }
    
    this.inUse.add(browser);
    return browser.newContext();
  }
  
  async release(context: BrowserContext) {
    await context.close();
    const browser = this.pool.find(b => b.contexts().includes(context));
    if (browser) this.inUse.delete(browser);
  }
}

export const browserPool = new BrowserPool();
```

**Benef√≠cios:**
- Reusa browsers (3x mais r√°pido)
- Controle de mem√≥ria
- Concorr√™ncia limitada

#### Dia 6: Auth no scraper
```typescript
// apps/scraper/src/middleware/auth.ts
import jwt from 'jsonwebtoken';

export function validateJWT(req, res, next) {
  const token = req.headers.authorization?.replace('Bearer ', '');
  
  try {
    const payload = jwt.verify(token, process.env.JWT_SECRET);
    req.userId = payload.sub;
    next();
  } catch {
    res.status(401).json({ error: 'Unauthorized' });
  }
}

// apps/web/app/api/search/create/route.ts
const token = jwt.sign({ sub: user.id }, process.env.JWT_SECRET);

await fetch(`${SCRAPER_URL}/api/scrape`, {
  headers: { Authorization: `Bearer ${token}` }
});
```

#### Dia 7: Logs estruturados
```typescript
// apps/scraper/src/lib/logger.ts
import pino from 'pino';

export const logger = pino({
  level: process.env.LOG_LEVEL || 'info',
  transport: process.env.NODE_ENV === 'development' 
    ? { target: 'pino-pretty' }
    : undefined
});

// Uso:
logger.info({ searchId, query, userId }, 'Search started');
logger.error({ err, searchId }, 'Scraping failed');
```

### Semana 2: Database & Performance

**Objetivo:** Queries r√°pidas mesmo com 100k+ empresas

#### Dia 8: Adicionar √≠ndices cr√≠ticos
```sql
-- Script: migrations/add-critical-indexes.sql

-- CNPJ (mais importante)
CREATE INDEX IF NOT EXISTS idx_companies_cnpj 
ON companies(cnpj) 
WHERE cnpj IS NOT NULL;

-- Queries por usu√°rio
CREATE INDEX IF NOT EXISTS idx_companies_user_search 
ON companies(search_id, created_at DESC);

-- Filtros comuns
CREATE INDEX IF NOT EXISTS idx_companies_rating 
ON companies(rating) 
WHERE rating IS NOT NULL;

CREATE INDEX IF NOT EXISTS idx_companies_has_website 
ON companies(website) 
WHERE website IS NOT NULL;

-- ANALYZE para atualizar estat√≠sticas
ANALYZE companies;
```

#### Dia 9-10: Normalizar tabela companies
```sql
-- Script: migrations/normalize-companies.sql

-- Nova tabela para dados Receita Federal
CREATE TABLE companies_receita (
  company_id UUID PRIMARY KEY REFERENCES companies(id) ON DELETE CASCADE,
  
  -- Identifica√ß√£o
  razao_social TEXT,
  nome_fantasia TEXT,
  situacao_cadastral TEXT,
  data_inicio_atividade DATE,
  
  -- Classifica√ß√£o
  porte_empresa TEXT,
  natureza_juridica TEXT,
  capital_social TEXT,
  
  -- CNAEs
  cnae_principal TEXT,
  cnaes_secundarios JSONB,
  
  -- Regime
  opcao_simples TEXT,
  opcao_mei TEXT,
  
  -- QSA
  qsa JSONB,
  
  -- Endere√ßo
  receita_logradouro TEXT,
  receita_numero TEXT,
  receita_cep TEXT,
  receita_municipio TEXT,
  receita_uf TEXT,
  
  -- Meta
  receita_fetched_at TIMESTAMP WITH TIME ZONE,
  
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_companies_receita_company ON companies_receita(company_id);
CREATE INDEX idx_companies_receita_razao ON companies_receita(razao_social);

-- Migrar dados existentes
INSERT INTO companies_receita (company_id, razao_social, ...)
SELECT id, razao_social, ...
FROM companies
WHERE razao_social IS NOT NULL;

-- Remover colunas antigas (ap√≥s valida√ß√£o)
-- ALTER TABLE companies DROP COLUMN razao_social;
-- ... etc
```

#### Dia 11: Implementar soft delete
```sql
-- Script: migrations/add-soft-delete.sql

ALTER TABLE companies ADD COLUMN deleted_at TIMESTAMP;
ALTER TABLE searches ADD COLUMN deleted_at TIMESTAMP;

CREATE INDEX idx_companies_active 
ON companies(deleted_at) 
WHERE deleted_at IS NULL;

CREATE INDEX idx_searches_active 
ON searches(deleted_at) 
WHERE deleted_at IS NULL;

-- Atualizar RLS policies
DROP POLICY IF EXISTS companies_read ON companies;
CREATE POLICY companies_read ON companies 
  FOR SELECT 
  TO authenticated 
  USING (
    deleted_at IS NULL AND
    search_id IN (
      SELECT id FROM searches 
      WHERE user_id = auth.uid() AND deleted_at IS NULL
    )
  );
```

#### Dia 12: Cache layer Redis
```typescript
// apps/web/lib/redis.ts
import Redis from 'ioredis';

export const redis = new Redis(process.env.REDIS_URL);

// apps/web/app/api/search/[searchId]/route.ts
export async function GET(req, { params }) {
  const { searchId } = params;
  
  // Check cache first
  const cached = await redis.get(`search:${searchId}`);
  if (cached) {
    return NextResponse.json(JSON.parse(cached));
  }
  
  // Fetch from DB
  const data = await supabase.from('searches').select('*').eq('id', searchId).single();
  
  // Cache for 30 minutes
  await redis.setex(`search:${searchId}`, 1800, JSON.stringify(data));
  
  return NextResponse.json(data);
}
```

#### Dia 13-14: Particionamento
```sql
-- Script: migrations/partition-companies.sql

-- Converter para particionada
CREATE TABLE companies_new (
  LIKE companies INCLUDING ALL
) PARTITION BY RANGE (created_at);

-- Criar parti√ß√µes mensais
CREATE TABLE companies_2025_11 PARTITION OF companies_new
  FOR VALUES FROM ('2025-11-01') TO ('2025-12-01');

CREATE TABLE companies_2025_12 PARTITION OF companies_new
  FOR VALUES FROM ('2025-12-01') TO ('2026-01-01');

-- Migrar dados (em lotes)
INSERT INTO companies_new SELECT * FROM companies LIMIT 10000;

-- Renomear (ap√≥s valida√ß√£o)
-- ALTER TABLE companies RENAME TO companies_old;
-- ALTER TABLE companies_new RENAME TO companies;
```

### Semana 3: Frontend & UX

**Objetivo:** Interface robusta e test√°vel

#### Dia 15-17: Refatorar ResultsTable
```typescript
// apps/web/app/home/scout/chat/_components/results/

// CompanyCard.tsx (150 linhas)
export function CompanyCard({ company }: { company: Company }) {
  return (
    <Collapsible>
      <CompanyHeader company={company} />
      <CollapsibleContent>
        <CompanyDetails company={company} />
      </CollapsibleContent>
    </Collapsible>
  );
}

// CompanyHeader.tsx (80 linhas)
export function CompanyHeader({ company }: { company: Company }) {
  return (
    <div className="flex items-center gap-4">
      <CompanyAvatar name={company.name} />
      <CompanyInfo company={company} />
      <RatingBadge rating={company.rating} count={company.reviews_count} />
      <CnpjBadge cnpj={company.cnpj} />
      <QuickActions company={company} />
    </div>
  );
}

// CompanyDetails.tsx (200 linhas)
export function CompanyDetails({ company }: { company: Company }) {
  return (
    <div className="grid grid-cols-2 gap-6">
      <GoogleBusinessInfo company={company} />
      <StrategicAnalysis company={company} />
    </div>
  );
}

// ReviewsAnalysis.tsx (150 linhas)
export function ReviewsAnalysis({ reviews }: { reviews: Review[] }) {
  const sentiment = useSentimentAnalysis(reviews);
  
  return (
    <div>
      <ReviewsSummary rating={...} distribution={...} />
      <SentimentAnalysis positive={...} negative={...} />
      <FrequentMentions words={...} />
    </div>
  );
}

// ImageGallery.tsx (80 linhas)
export function ImageGallery({ images }: { images: string[] }) {
  const [position, setPosition] = useState(0);
  
  return (
    <div className="relative">
      <ScrollContainer images={images} position={position} />
      <NavigationButtons onPrev={...} onNext={...} />
    </div>
  );
}
```

#### Dia 18: Error boundaries e tratamento
```typescript
// apps/web/app/home/scout/chat/error.tsx
'use client';

export default function Error({ error, reset }: {
  error: Error;
  reset: () => void;
}) {
  return (
    <div className="p-6 text-center">
      <h2>Algo deu errado</h2>
      <p>{error.message}</p>
      <Button onClick={reset}>Tentar novamente</Button>
    </div>
  );
}

// apps/web/app/home/scout/chat/page.tsx
import { Suspense } from 'react';

export default function ChatPage() {
  return (
    <ErrorBoundary fallback={<ErrorFallback />}>
      <Suspense fallback={<LoadingSkeleton />}>
        <ChatContent />
      </Suspense>
    </ErrorBoundary>
  );
}
```

#### Dia 19: Polling inteligente
```typescript
// packages/features/kaix-scout/src/hooks/use-search.ts

export function useSearch(searchId: string | null) {
  const [retries, setRetries] = useState(0);
  const maxRetries = 40; // 2 minutos (40 * 3s)
  
  return useQuery({
    queryKey: ['search', searchId],
    queryFn: () => getSearch(searchId!),
    enabled: !!searchId,
    refetchInterval: (query) => {
      const data = query.state.data;
      
      // Timeout ap√≥s 2 minutos
      if (retries >= maxRetries) {
        console.error('Search timeout after 2 minutes');
        return false;
      }
      
      if (data?.search.status === 'processing') {
        setRetries(prev => prev + 1);
        return 3000;
      }
      
      setRetries(0);
      return false;
    },
    onError: () => {
      toast.error('Erro ao buscar resultados. Tente novamente.');
    },
  });
}
```

#### Dia 20: Unificar types
```typescript
// packages/types/src/company.ts

/**
 * Unified Company type
 * Combines Google Maps + Receita Federal data
 */
export interface Company {
  // IDs
  id: string;
  place_id: string;
  search_id: string;
  user_id: string;
  
  // Google Maps - Basic
  name: string;
  address: string;
  phone?: string;
  website?: string;
  categories: string[];
  google_maps_link: string;
  
  // Google Maps - Location
  latitude: number;
  longitude: number;
  plus_code?: string;
  
  // Google Maps - Reviews
  rating?: number;
  reviews_count?: number;
  top_reviews?: Review[];
  
  // Google Maps - Rich Data
  opening_hours?: string;
  about?: string;
  images?: string[];
  price_level?: number;
  
  // Receita Federal
  cnpj?: string;
  razao_social?: string;
  nome_fantasia?: string;
  situacao_cadastral?: string;
  porte_empresa?: string;
  capital_social?: string;
  cnae_principal?: string;
  qsa?: QSA[];
  
  // Meta
  status: 'pending' | 'analyzing' | 'analyzed' | 'error';
  created_at: string;
  updated_at: string;
  deleted_at?: string;
}

// Remover tipos antigos:
// - GoogleMapsPlace
// - Place
// - Company (antigo)
```

### Semana 4: Observability & Deploy

**Objetivo:** Sistema monitor√°vel e deploy automatizado

#### Dia 21-23: Testes b√°sicos
```typescript
// packages/features/kaix-scout/src/hooks/use-search.test.ts
import { renderHook, waitFor } from '@testing-library/react';
import { QueryClient, QueryClientProvider } from '@tanstack/react-query';
import { useSearch } from './use-search';

describe('useSearch', () => {
  it('should poll until completed', async () => {
    const queryClient = new QueryClient();
    
    const { result } = renderHook(
      () => useSearch('search-id'),
      {
        wrapper: ({ children }) => (
          <QueryClientProvider client={queryClient}>
            {children}
          </QueryClientProvider>
        ),
      }
    );
    
    await waitFor(() => {
      expect(result.current.data?.search.status).toBe('completed');
    }, { timeout: 10000 });
  });
  
  it('should timeout after 2 minutes', async () => {
    // Mock never completing
    jest.mock('./api', () => ({
      getSearch: () => Promise.resolve({ search: { status: 'processing' } })
    }));
    
    const { result } = renderHook(() => useSearch('search-id'));
    
    await waitFor(() => {
      expect(result.current.isError).toBe(true);
    }, { timeout: 125000 });
  });
});

// apps/scraper/src/services/google-maps.test.ts
import { scrapeGoogleMaps } from './google-maps.service';

describe('scrapeGoogleMaps', () => {
  it('should extract 12 places', async () => {
    const result = await scrapeGoogleMaps('restaurantes em S√£o Paulo', 12);
    
    expect(result.places).toHaveLength(12);
    expect(result.places[0]).toHaveProperty('name');
    expect(result.places[0]).toHaveProperty('address');
  });
});
```

#### Dia 24-25: CI/CD GitHub Actions
```yaml
# .github/workflows/test-and-deploy.yml
name: Test and Deploy

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - uses: pnpm/action-setup@v2
        with:
          version: 8
      
      - uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'pnpm'
      
      - run: pnpm install
      
      - run: pnpm run lint
      
      - run: pnpm run typecheck
      
      - run: pnpm run test
      
      - name: Build
        run: pnpm run build
  
  deploy-web:
    needs: test
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Deploy to Vercel
        uses: amondnet/vercel-action@v20
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          vercel-org-id: ${{ secrets.VERCEL_ORG_ID }}
          vercel-project-id: ${{ secrets.VERCEL_PROJECT_ID }}
  
  deploy-scraper:
    needs: test
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Build Docker image
        run: docker build -t scraper:latest ./apps/scraper
      
      - name: Push to Registry
        run: |
          echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
          docker push scraper:latest
```

#### Dia 26: Monitoring Sentry
```typescript
// apps/web/instrumentation.ts
import * as Sentry from '@sentry/nextjs';

Sentry.init({
  dsn: process.env.SENTRY_DSN,
  environment: process.env.NODE_ENV,
  tracesSampleRate: 1.0,
  integrations: [
    new Sentry.BrowserTracing(),
    new Sentry.Replay(),
  ],
});

// apps/scraper/src/index.ts
import * as Sentry from '@sentry/node';

Sentry.init({
  dsn: process.env.SENTRY_DSN,
  tracesSampleRate: 1.0,
});

// Capture errors
app.use(Sentry.Handlers.errorHandler());
```

#### Dia 27: Healthchecks robustos
```typescript
// apps/scraper/src/routes/health.route.ts

export async function healthCheck(req: Request, res: Response) {
  const checks = {
    status: 'ok',
    timestamp: new Date().toISOString(),
    uptime: process.uptime(),
    services: {
      database: 'checking',
      redis: 'checking',
      browserPool: 'checking',
    }
  };
  
  try {
    // Check database
    await supabase.from('searches').select('count').limit(1);
    checks.services.database = 'ok';
  } catch {
    checks.services.database = 'error';
    checks.status = 'degraded';
  }
  
  try {
    // Check Redis
    await redis.ping();
    checks.services.redis = 'ok';
  } catch {
    checks.services.redis = 'error';
    checks.status = 'degraded';
  }
  
  try {
    // Check browser pool
    const available = browserPool.availableCount();
    checks.services.browserPool = `${available}/${browserPool.maxSize} available`;
  } catch {
    checks.services.browserPool = 'error';
    checks.status = 'degraded';
  }
  
  const statusCode = checks.status === 'ok' ? 200 : 503;
  res.status(statusCode).json(checks);
}
```

#### Dia 28-30: Documentation
```markdown
# docs/ARCHITECTURE.md

## System Architecture

```mermaid
graph TB
    User[User] --> NextJS[Next.js App]
    NextJS --> API[API Routes]
    API --> Queue[Bull Queue]
    Queue --> Scraper[Scraper Service]
    Scraper --> Browser[Browser Pool]
    Browser --> GoogleMaps[Google Maps]
    Scraper --> CNPJ[CNPJ Service]
    CNPJ --> Receita[Receita Federal API]
    API --> Supabase[(Supabase)]
    Scraper --> Supabase
    API --> Redis[(Redis Cache)]
```

## API Reference

### POST /api/search/create

Create a new search job.

**Request:**
```json
{
  "query": "restaurantes em S√£o Paulo",
  "maxPlaces": 12
}
```

**Response:**
```json
{
  "success": true,
  "data": {
    "searchId": "uuid",
    "status": "processing"
  }
}
```

### GET /api/search/{searchId}

Get search status and results.

**Response:**
```json
{
  "success": true,
  "data": {
    "search": {
      "id": "uuid",
      "status": "completed",
      "total_results": 12
    },
    "companies": [...],
    "progress": {
      "current": 12,
      "total": 12,
      "percentage": 100
    }
  }
}
```

## Deployment

### Prerequisites
- Node.js 18+
- PostgreSQL 15+
- Redis 7+
- Docker

### Environment Variables

```env
# Database
DATABASE_URL=postgresql://...
SUPABASE_URL=https://...
SUPABASE_ANON_KEY=...

# Redis
REDIS_URL=redis://localhost:6379

# Scraper
SCRAPER_URL=http://localhost:3001
JWT_SECRET=...

# Monitoring
SENTRY_DSN=...
```

### Deploy Steps

1. Build apps
```bash
pnpm install
pnpm run build
```

2. Run migrations
```bash
pnpm run supabase:web:reset
```

3. Start services
```bash
docker-compose up -d
pnpm run start
```
```

---

## üìä RESUMO EXECUTIVO

### Estado Atual: **MVP Funcional mas Fr√°gil**

#### Pontos Fortes ‚úÖ
- Scraper r√°pido e eficiente (2-3s para 12 lugares)
- UI moderna e responsiva
- Integra√ß√£o Receita Federal funcionando
- Schema de banco bem pensado
- TanStack Query bem implementado

#### Pontos Fracos Cr√≠ticos ‚ùå
- Arquitetura n√£o aguenta escala (>10 usu√°rios = crash)
- Zero testes, zero monitoring
- Scraper fora do monorepo (deploy hell)
- Background jobs sem queue/retry
- Database sem √≠ndices cr√≠ticos
- Frontend polling infinito (memory leak)
- API scraper sem autentica√ß√£o

### Veredicto Final

Sistema **N√ÉO EST√Å PRONTO para produ√ß√£o com usu√°rios reais**.

**Necessita de 15-30 dias de refatora√ß√£o** focada em:

1. **Job queue + browser pool** (Semana 1)
2. **√çndices database + normaliza√ß√£o** (Semana 2)
3. **Refatora√ß√£o frontend** (Semana 3)
4. **Testes b√°sicos + CI/CD** (Semana 4)

**Depois disso:** Sistema aguenta 100+ usu√°rios simult√¢neos e pode crescer incrementalmente.

### M√©tricas de Sa√∫de

| Aspecto | Nota | Status |
|---------|------|--------|
| **Frontend** | 6/10 | üü° M√©dio |
| **Backend** | 4/10 | üî¥ Cr√≠tico |
| **Database** | 5/10 | üü† Alto |
| **Qualidade** | 5/10 | üü† Alto |
| **Deploy** | 3/10 | üî¥ Cr√≠tico |
| **Monitoring** | 2/10 | üî¥ Cr√≠tico |
| **Testes** | 0/10 | üî¥ Cr√≠tico |
| **GERAL** | **5/10** | **üü† N√£o Produ√ß√£o-Ready** |

### Pr√≥ximos Passos Imediatos

**Antes de adicionar features:**
1. ‚úÖ Implementar job queue (BullMQ)
2. ‚úÖ Browser pool gerenciado
3. ‚úÖ Auth na API scraper
4. ‚úÖ √çndice em companies.cnpj
5. ‚úÖ Polling com timeout
6. ‚úÖ Logs estruturados
7. ‚úÖ Testes b√°sicos

**Somente depois:**
- Features de IA
- An√°lise de sentimento avan√ßada
- CRM integrado
- API p√∫blica

---

**Elaborado por:** GitHub Copilot (Claude Sonnet 4.5)  
**Data:** 28 de novembro de 2025  
**Vers√£o:** 1.0  
**Confidencial:** Uso interno apenas

---

*Este diagn√≥stico t√©cnico √© baseado em an√°lise do c√≥digo-fonte real e representa o estado atual do projeto. As recomenda√ß√µes s√£o prioritizadas para maximizar estabilidade e escalabilidade antes de crescimento.*
